{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code was compiled by Matthew Demeter, Duncan Boynton, and Grant Salzsiedler, but we had help from the following sources, providing large snippets of code, insight on how to train an LSTM autoencoder, and more. Most of this code is not original, but rather bits and pieces from these articles and minor tweaks. Our sole objectives were to create a working model and learn more about how these models work, how they are trained, and what key challenges are in developing and testing these models. \n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/\n",
        "\n",
        "https://blog.paperspace.com/introduction-to-seq2seq-models/\n",
        "\n",
        "https://blog.paperspace.com/implement-seq2seq-for-text-summarization-keras/\n",
        "\n",
        "https://towardsdatascience.com/text-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e?gi=f9cd15db2ccd"
      ],
      "metadata": {
        "id": "CuQUPaMGF-jv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCfyo8NYncCX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from attention import AttentionLayer"
      ],
      "metadata": {
        "id": "Ad1Ws4SDhIec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# load from file\n",
        "stories = pickle.load(open('/content/drive/MyDrive/cnn_dataset.pkl', 'rb'))\n",
        "print('Loaded Stories %d' % len(stories))"
      ],
      "metadata": {
        "id": "RL-t0Uclnirp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# required for rest of notebook to run\n",
        "# primarily using Keras and Tensorflow libraries\n",
        "\n",
        "!pip install tf\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "import random\n",
        "import requests as rq\n",
        "import sys\n",
        "import io\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.layers import Input, Embedding, TimeDistributed, RepeatVector, Concatenate\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from collections import Counter\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from datetime import datetime\n",
        "import keras\n",
        "import keras.callbacks\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "%load_ext tensorboard\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "TagnA6SBnjkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(stories))"
      ],
      "metadata": {
        "id": "uTFFK6MCiOOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is just a safety measure before manipulating the articles, creating\n",
        "# a deep copy that can be referenced any time\n",
        "import copy\n",
        "stories2 = copy.deepcopy(stories)\n",
        "for story in stories:\n",
        "  story['story'] = \" \".join(story['story'])\n",
        "  story['highlights'] = \" \".join(story['highlights'])\n",
        "for i in range(len(stories)): \n",
        "  stories[i]['highlights'] = stories2[i]['highlights'][0]\n",
        "\n",
        "articles = []\n",
        "sums = []\n",
        "\n",
        "# creating list object with strings of each article and then summaries\n",
        "# note for our current dataset, \"highlights\" are summaries of the story \n",
        "for story in stories:\n",
        "  articles.append(story['story'])\n",
        "  sums.append(story['highlights'])"
      ],
      "metadata": {
        "id": "6bPjeU-pnliB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# useful for seeing the length of different stories and how many\n",
        "# fall under an arbitrary threshold. 80 can be changed to any\n",
        "# number but recommended to keep article length low\n",
        "cnt = 0\n",
        "for i in articles:\n",
        "    if len(i.split()) <= 80:\n",
        "      cnt = cnt + 1\n",
        "print(cnt / len(articles))\n",
        "\n",
        "# src and sum text length. src is the news article length\n",
        "# this code could be improved by not only taking articles in this \n",
        "# length but actually taking all of them and just cutting off words\n",
        "# above the src_txt_length word count\n",
        "src_txt_length = 100\n",
        "sum_txt_length = 18"
      ],
      "metadata": {
        "id": "N35RiYi7YYOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# puts articles and summaries into np array\n",
        "cleaned_text = np.array(articles)\n",
        "cleaned_summary= np.array(sums)\n",
        "\n",
        "short_text = []\n",
        "short_summary = []\n",
        "\n",
        "# filters out articles and summaries that are too long. this is where improvement\n",
        "# could be made to include longer articles and just cut them off\n",
        "for i in range(len(cleaned_text)):\n",
        "    if len(cleaned_summary[i].split()) <= sum_txt_length and len(cleaned_text[i].split()) <= src_txt_length:\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "post_pre = pd.DataFrame({'text': short_text,'summary': short_summary})\n",
        "\n",
        "# Add tokens labeled 'sostok' and 'eostok' to guide autoencoder through its \n",
        "# summery generation\n",
        "post_pre['summary'] = post_pre['summary'].apply(lambda x: 'sostok ' + x + ' eostok')\n",
        "\n",
        "post_pre.head(5)"
      ],
      "metadata": {
        "id": "Apej8ju3apkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standard train test split of data for training and then performance testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(\n",
        "    np.array(post_pre[\"text\"]),\n",
        "    np.array(post_pre[\"summary\"]),\n",
        "    test_size=0.1,\n",
        "    random_state=0,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "GgYfAGXeNsMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# threshold that determines if a word is common enough to tokenized \n",
        "thresh = 3\n",
        "\n",
        "cnt = 0\n",
        "tot_cnt = 0\n",
        "\n",
        "for key, value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt = tot_cnt + 1\n",
        "    if value < thresh:\n",
        "        cnt = cnt + 1\n",
        "    \n",
        "# how many words don't meet the threshold\n",
        "print(\"% of rare words in vocabulary: \", (cnt / tot_cnt) * 100)\n",
        "\n",
        "# create first tokenizer that ignores \"rare\" words\n",
        "x_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "# transform text into numbers\n",
        "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "# Padding for uniform size\n",
        "x_tr = pad_sequences(x_tr_seq,  maxlen=src_txt_length, padding='post')\n",
        "x_val = pad_sequences(x_val_seq, maxlen=src_txt_length, padding='post')\n",
        "\n",
        "# Size of vocabulary (+1 for padding token sostok and eostok)\n",
        "x_voc = x_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in X = {}\".format(x_voc))\n"
      ],
      "metadata": {
        "id": "eOXjm8tOnoJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basically do the same thing but for the summaries\n",
        "# \n",
        "# it is beneficial to create separate tokenizers\n",
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "thresh = 3\n",
        "\n",
        "cnt = 0\n",
        "tot_cnt = 0\n",
        "\n",
        "for key, value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt = tot_cnt + 1\n",
        "    if value < thresh:\n",
        "        cnt = cnt + 1\n",
        "\n",
        "print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n",
        "\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "y_tr = pad_sequences(y_tr_seq, maxlen=sum_txt_length, padding='post')\n",
        "y_val = pad_sequences(y_val_seq, maxlen=sum_txt_length, padding='post')\n",
        "\n",
        "\n",
        "y_voc = y_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
      ],
      "metadata": {
        "id": "59qk-tUxjMBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K \n",
        "K.clear_session() \n",
        "\n",
        "latent_dim = 200\n",
        "embedding_dim = 300\n",
        "\n",
        "# ENCODER STARTS HERE\n",
        "encoder_inputs = Input(shape=(src_txt_length, ))\n",
        "\n",
        "# Embedding \n",
        "enc_emb = Embedding(x_voc, embedding_dim,\n",
        "                    trainable=True)(encoder_inputs)\n",
        "\n",
        "# LSTM 1\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n",
        "                     return_state=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
        "\n",
        "# LSTM 2\n",
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n",
        "                     return_state=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# LSTM 3\n",
        "encoder_lstm3 = LSTM(latent_dim, return_state=True,\n",
        "                     return_sequences=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using encoder_states as the initial state\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "\n",
        "# Embedding\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Decoder LSTM\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
        "                    return_state=True, dropout=0.4,\n",
        "                    recurrent_dropout=0.2)\n",
        "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
        "    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "#Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer') \n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
        "\n",
        "# Concat attention output and decoder LSTM output \n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "H4KQuVXfn6Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
      ],
      "metadata": {
        "id": "z5Mb9Bf_SLxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Need high ram colab Pro\n",
        "history = model.fit(\n",
        "    [x_tr, y_tr[:, :-1]],\n",
        "    y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:],\n",
        "    epochs=20,\n",
        "    callbacks=[es],\n",
        "    batch_size=512,\n",
        "    validation_data=([x_val, y_val[:, :-1]],\n",
        "                     y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:\n",
        "                     , 1:]),\n",
        "    )"
      ],
      "metadata": {
        "id": "vyi5ffcB6cLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional code to save your model after training\n",
        "\n",
        "#model.save('/content/drive/MyDrive/model4')"
      ],
      "metadata": {
        "id": "v1FzYZxG6bAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code to access your model once it has been trained\n",
        "#saved_model = keras.models.load_model('/content/drive/MyDrive/model2')\n",
        "#model = saved_model"
      ],
      "metadata": {
        "id": "zo7Y6MQgbZRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping back from the outputted summary in tokenized form to word form\n",
        "reverse_target_word_index = y_tokenizer.index_word\n",
        "reverse_source_word_index = x_tokenizer.index_word\n",
        "target_word_index = y_tokenizer.word_index\n"
      ],
      "metadata": {
        "id": "dahUr8OlUBy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder inference\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# decoder inference\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(src_txt_length,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "[decoder_outputs2] + [state_h2, state_c2])"
      ],
      "metadata": {
        "id": "Vyfpo3HOVFvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Chose the 'start' word as the first word of the target sequence\n",
        "    target_seq[0, 0] = target_word_index['start']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index+1]\n",
        "\n",
        "        if(sampled_token!='end'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "            # Exit condition: either hit max length or find stop word.\n",
        "            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (sum_txt_length-1)):\n",
        "                stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "s2P2Wj2JVZVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
        "        newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if(i!=0):\n",
        "        newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "metadata": {
        "id": "7HQilVuPVfpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(25, 30):\n",
        "  print(\"Review:\",seq2text(x_val[i]))\n",
        "  print(\"Original summary:\",seq2summary(y_val[i]))\n",
        "  print(\"Predicted summary:\",decode_sequence(x_val[i].reshape(1,src_txt_length)))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "GR33u4rKViSW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UsXOg-gT-1JC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}