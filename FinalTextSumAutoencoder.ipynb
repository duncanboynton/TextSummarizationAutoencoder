{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duncanboynton/TextSummarizationAutoencoder/blob/main/FinalTextSumAutoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code was compiled by Matthew Demeter, Duncan Boynton, and Grant Salzsiedler, but we had help from the following sources, providing large snippets of code, insight on how to train an LSTM autoencoder, and more. Most of this code is not original, but rather bits and pieces from these articles and minor tweaks. Our sole objectives were to create a working model and learn more about how these models work, how they are trained, and what key challenges are in developing and testing these models. \n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/\n",
        "\n",
        "https://blog.paperspace.com/introduction-to-seq2seq-models/\n",
        "\n",
        "https://blog.paperspace.com/implement-seq2seq-for-text-summarization-keras/\n",
        "\n",
        "https://towardsdatascience.com/text-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e?gi=f9cd15db2ccd"
      ],
      "metadata": {
        "id": "CuQUPaMGF-jv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCfyo8NYncCX",
        "outputId": "d5bc1b7d-0434-41e4-f05f-b1ef78c079f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from attention import AttentionLayer"
      ],
      "metadata": {
        "id": "Ad1Ws4SDhIec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# load from file\n",
        "stories = pickle.load(open('/content/drive/MyDrive/cnn_dataset.pkl', 'rb'))\n",
        "print('Loaded Stories %d' % len(stories))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL-t0Uclnirp",
        "outputId": "c72f9bcf-ffcd-4a6c-d907-aad7759358b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Stories 92579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# required for rest of notebook to run\n",
        "# primarily using Keras and Tensorflow libraries\n",
        "\n",
        "!pip install tf\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "import random\n",
        "import requests as rq\n",
        "import sys\n",
        "import io\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.layers import Input, Embedding, TimeDistributed, RepeatVector, Concatenate\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from collections import Counter\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from datetime import datetime\n",
        "import keras\n",
        "import keras.callbacks\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "%load_ext tensorboard\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TagnA6SBnjkJ",
        "outputId": "c6d749c7-a74b-4ec4-a14d-cc07111595d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(stories))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTFFK6MCiOOe",
        "outputId": "3369b0bd-3f1a-4e9b-c701-aae455432c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is just a safety measure before manipulating the articles, creating\n",
        "# a deep copy that can be referenced any time\n",
        "import copy\n",
        "stories2 = copy.deepcopy(stories)\n",
        "for story in stories:\n",
        "  story['story'] = \" \".join(story['story'])\n",
        "  story['highlights'] = \" \".join(story['highlights'])\n",
        "for i in range(len(stories)): \n",
        "  stories[i]['highlights'] = stories2[i]['highlights'][0]\n",
        "\n",
        "articles = []\n",
        "sums = []\n",
        "\n",
        "# creating list object with strings of each article and then summaries\n",
        "# note for our current dataset, \"highlights\" are summaries of the story \n",
        "for story in stories:\n",
        "  articles.append(story['story'])\n",
        "  sums.append(story['highlights'])"
      ],
      "metadata": {
        "id": "6bPjeU-pnliB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# useful for seeing the length of different stories and how many\n",
        "# fall under an arbitrary threshold. 80 can be changed to any\n",
        "# number but recommended to keep article length low\n",
        "cnt = 0\n",
        "for i in articles:\n",
        "    if len(i.split()) <= 80:\n",
        "      cnt = cnt + 1\n",
        "print(cnt / len(articles))\n",
        "\n",
        "# src and sum text length. src is the news article length\n",
        "# this code could be improved by not only taking articles in this \n",
        "# length but actually taking all of them and just cutting off words\n",
        "# above the src_txt_length word count\n",
        "src_txt_length = 100\n",
        "sum_txt_length = 18"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N35RiYi7YYOC",
        "outputId": "f9a48f7c-9e13-409f-dff3-40183d071868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0037265470571079835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# puts articles and summaries into np array\n",
        "cleaned_text = np.array(articles)\n",
        "cleaned_summary= np.array(sums)\n",
        "\n",
        "short_text = []\n",
        "short_summary = []\n",
        "\n",
        "# filters out articles and summaries that are too long. this is where improvement\n",
        "# could be made to include longer articles and just cut them off\n",
        "for i in range(len(cleaned_text)):\n",
        "    if len(cleaned_summary[i].split()) <= sum_txt_length and len(cleaned_text[i].split()) <= src_txt_length:\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "post_pre = pd.DataFrame({'text': short_text,'summary': short_summary})\n",
        "\n",
        "# Add tokens labeled 'sostok' and 'eostok' to guide autoencoder through its \n",
        "# summery generation\n",
        "post_pre['summary'] = post_pre['summary'].apply(lambda x: 'sostok ' + x + ' eostok')\n",
        "\n",
        "post_pre.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "Apej8ju3apkC",
        "outputId": "45d85595-209d-4cf9-cad7-db230a6ac726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  as barack obama makes his case to the nation f...   \n",
              "1                                                      \n",
              "\n",
              "                                             summary  \n",
              "0  sostok barack obama making case to nation for ...  \n",
              "1  sostok cnncom will feature ireporter photos in...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-caf1fad7-d4f5-4c85-bd71-69c66623ddee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>as barack obama makes his case to the nation f...</td>\n",
              "      <td>sostok barack obama making case to nation for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>sostok cnncom will feature ireporter photos in...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-caf1fad7-d4f5-4c85-bd71-69c66623ddee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-caf1fad7-d4f5-4c85-bd71-69c66623ddee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-caf1fad7-d4f5-4c85-bd71-69c66623ddee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# standard train test split of data for training and then performance testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(\n",
        "    np.array(post_pre[\"text\"]),\n",
        "    np.array(post_pre[\"summary\"]),\n",
        "    test_size=0.1,\n",
        "    random_state=0,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "GgYfAGXeNsMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# threshold that determines if a word is common enough to tokenized \n",
        "thresh = 3\n",
        "\n",
        "cnt = 0\n",
        "tot_cnt = 0\n",
        "\n",
        "for key, value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt = tot_cnt + 1\n",
        "    if value < thresh:\n",
        "        cnt = cnt + 1\n",
        "    \n",
        "# how many words don't meet the threshold\n",
        "print(\"% of rare words in vocabulary: \", (cnt / tot_cnt) * 100)\n",
        "\n",
        "# create first tokenizer that ignores \"rare\" words\n",
        "x_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "# transform text into numbers\n",
        "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "# Padding for uniform size\n",
        "x_tr = pad_sequences(x_tr_seq,  maxlen=src_txt_length, padding='post')\n",
        "x_val = pad_sequences(x_val_seq, maxlen=src_txt_length, padding='post')\n",
        "\n",
        "# Size of vocabulary (+1 for padding token sostok and eostok)\n",
        "x_voc = x_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in X = {}\".format(x_voc))\n"
      ],
      "metadata": {
        "id": "eOXjm8tOnoJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4e059a-d2ff-444c-f7b0-292637588f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of rare words in vocabulary:  73.02965754546943\n",
            "Size of vocabulary in X = 1647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basically do the same thing but for the summaries\n",
        "# \n",
        "# it is beneficial to create separate tokenizers\n",
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "thresh = 3\n",
        "\n",
        "cnt = 0\n",
        "tot_cnt = 0\n",
        "\n",
        "for key, value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt = tot_cnt + 1\n",
        "    if value < thresh:\n",
        "        cnt = cnt + 1\n",
        "\n",
        "print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n",
        "\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "y_tr = pad_sequences(y_tr_seq, maxlen=sum_txt_length, padding='post')\n",
        "y_val = pad_sequences(y_val_seq, maxlen=sum_txt_length, padding='post')\n",
        "\n",
        "\n",
        "y_voc = y_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59qk-tUxjMBn",
        "outputId": "845abd2d-e4e4-4085-c478-5adfb70e0c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of rare words in vocabulary: 84.83735571878279\n",
            "Size of vocabulary in Y = 290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K \n",
        "K.clear_session() \n",
        "\n",
        "latent_dim = 200\n",
        "embedding_dim = 300\n",
        "\n",
        "# ENCODER STARTS HERE\n",
        "encoder_inputs = Input(shape=(src_txt_length, ))\n",
        "\n",
        "# Embedding \n",
        "enc_emb = Embedding(x_voc, embedding_dim,\n",
        "                    trainable=True)(encoder_inputs)\n",
        "\n",
        "# LSTM 1\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n",
        "                     return_state=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
        "\n",
        "# LSTM 2\n",
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n",
        "                     return_state=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# LSTM 3\n",
        "encoder_lstm3 = LSTM(latent_dim, return_state=True,\n",
        "                     return_sequences=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using encoder_states as the initial state\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "\n",
        "# Embedding\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Decoder LSTM\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
        "                    return_state=True, dropout=0.4,\n",
        "                    recurrent_dropout=0.2)\n",
        "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
        "    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "#Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer') \n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
        "\n",
        "# Concat attention output and decoder LSTM output \n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "H4KQuVXfn6Ov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e21b7df0-c23d-457d-8b46-83f2fd125283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 100, 300)     494100      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 100, 200),   400800      ['embedding[0][0]']              \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 100, 200),   320800      ['lstm[0][0]']                   \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 300)    87000       ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 100, 200),   320800      ['lstm_1[0][0]']                 \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 200),  400800      ['embedding_1[0][0]',            \n",
            "                                 (None, 200),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 200)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " attention_layer (AttentionLaye  ((None, None, 200),  80200      ['lstm_2[0][0]',                 \n",
            " r)                              (None, None, 100))               'lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            " concat_layer (Concatenate)     (None, None, 400)    0           ['lstm_3[0][0]',                 \n",
            "                                                                  'attention_layer[0][0]']        \n",
            "                                                                                                  \n",
            " time_distributed (TimeDistribu  (None, None, 290)   116290      ['concat_layer[0][0]']           \n",
            " ted)                                                                                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,220,790\n",
            "Trainable params: 2,220,790\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
      ],
      "metadata": {
        "id": "z5Mb9Bf_SLxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Need high ram colab Pro\n",
        "history = model.fit(\n",
        "    [x_tr, y_tr[:, :-1]],\n",
        "    y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:],\n",
        "    epochs=20,\n",
        "    callbacks=[es],\n",
        "    batch_size=512,\n",
        "    validation_data=([x_val, y_val[:, :-1]],\n",
        "                     y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:\n",
        "                     , 1:]),\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyi5ffcB6cLQ",
        "outputId": "8d23d6f3-239f-4fa6-9532-fd7c6af62df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2/2 [==============================] - 29s 3s/step - loss: 5.6669 - val_loss: 5.2695\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 3s 2s/step - loss: 5.2873 - val_loss: 3.0807\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 5s 3s/step - loss: 3.0966 - val_loss: 2.7705\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 3s 2s/step - loss: 2.8149 - val_loss: 2.4614\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 3s 2s/step - loss: 2.5216 - val_loss: 2.5265\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 3s 2s/step - loss: 2.5699 - val_loss: 2.4499\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 5s 3s/step - loss: 2.5193 - val_loss: 2.3355\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 3s 2s/step - loss: 2.4003 - val_loss: 2.7129\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 3s 2s/step - loss: 2.7546 - val_loss: 2.2897\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 3s 1s/step - loss: 2.3651 - val_loss: 2.2761\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 5s 3s/step - loss: 2.3732 - val_loss: 2.2312\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 3s 1s/step - loss: 2.3207 - val_loss: 2.3303\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 3s 2s/step - loss: 2.4081 - val_loss: 2.2979\n",
            "Epoch 13: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optional code to save your model after training\n",
        "\n",
        "#model.save('/content/drive/MyDrive/model4')"
      ],
      "metadata": {
        "id": "v1FzYZxG6bAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code to access your model once it has been trained\n",
        "#saved_model = keras.models.load_model('/content/drive/MyDrive/model2')\n",
        "#model = saved_model"
      ],
      "metadata": {
        "id": "zo7Y6MQgbZRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping back from the outputted summary in tokenized form to word form\n",
        "reverse_target_word_index = y_tokenizer.index_word\n",
        "reverse_source_word_index = x_tokenizer.index_word\n",
        "target_word_index = y_tokenizer.word_index\n"
      ],
      "metadata": {
        "id": "dahUr8OlUBy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder inference\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# decoder inference\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(src_txt_length,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "[decoder_outputs2] + [state_h2, state_c2])"
      ],
      "metadata": {
        "id": "Vyfpo3HOVFvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Chose the 'start' word as the first word of the target sequence\n",
        "    target_seq[0, 0] = target_word_index['start']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index+1]\n",
        "\n",
        "        if(sampled_token!='end'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "            # Exit condition: either hit max length or find stop word.\n",
        "            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (sum_txt_length-1)):\n",
        "                stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "s2P2Wj2JVZVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
        "        newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if(i!=0):\n",
        "        newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "metadata": {
        "id": "7HQilVuPVfpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(25, 30):\n",
        "  print(\"Review:\",seq2text(x_val[i]))\n",
        "  print(\"Original summary:\",seq2summary(y_val[i]))\n",
        "  print(\"Predicted summary:\",decode_sequence(x_val[i].reshape(1,src_txt_length)))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "GR33u4rKViSW",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "390a3b90-f419-4c31-8255-d299af3720d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b30f01ab0110>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Review:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq2text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original summary:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq2summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted summary:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_txt_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-085da0858a1b>\u001b[0m in \u001b[0;36mseq2summary\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnewString\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m       \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mtarget_word_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mtarget_word_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mnewString\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewString\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mreverse_target_word_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnewString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'start'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UsXOg-gT-1JC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}